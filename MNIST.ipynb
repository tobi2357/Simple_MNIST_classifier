{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple MNIST classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this little notebook we will train neural networks on the MNIST dataset. We begin with a simple fully connected network and afterwards compare it to a convolutional neural network. For the latter we look at three different possibilies of changing the network architecture in order to improve model performance. We also do a little bit of error analysis and point at ways to improve the networks though we do not go in these direction at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the basic libraries and modules we are going to use. If you wish to train your model on an AMD graphics card, you may want to have a look at plaidml and change the first cell accordingly. Also if you don't want to use tensorboard feel free to delete the respective lines of code using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Input, Flatten, BatchNormalization, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_label(pred):\n",
    "    return np.argmax(pred)\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us load the MNIST dataset. Luckily, it is part of the keras datasets so we can just load it with a simple command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  60000\n",
      "Number of test samples:  10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print('Number of training samples: ', x_train.shape[0])\n",
    "print('Number of test samples: ', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset, let us have a look at the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
    "for ind, ax in enumerate(axes.flat):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(x_train[ind], cmap=plt.get_cmap('gray'))\n",
    "    ax.set_title('Label: ' + str(y_train[ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so these are pictures of handwritten images, each 28x28 pixels and with greyscale values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't much data preprocessing necessary for this dataset. But converting the pixel values to numbers between 0 and 1 is definitely something we can do easily and which speeds up training quite a bit. So let us do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y):\n",
    "    x = x.astype('float32')\n",
    "    x = x.reshape((x.shape[0], 28, 28, 1)) # add extra dimension to use conv2d in the model\n",
    "    x = x/255 # normalize to values between 0 and 1\n",
    "    \n",
    "    y = to_categorical(y) # one hot encoding\n",
    "    return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_data(x_train, y_train)\n",
    "x_test, y_test = preprocess_data(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by defining a fully connected network as first model. We take the input image (28x28x1), flatten it into a fully connected layer of size 28\\*28=784 and then reduce its size by a factor of four in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MNIST_fully_connected\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 472,042\n",
      "Trainable params: 472,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def fully_connected_model(input_shape, name='MNIST_fully_connected'):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    X = Flatten()(X) # fully connected layer with 784 nodes\n",
    "    \n",
    "    # reduce by factor of four in each layer\n",
    "    X = Dense(512, activation='relu')(X)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(10, activation='softmax')(X)\n",
    "\n",
    "    model = Model(X_input, X, name=name)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "fully_connected_model = fully_connected_model((28, 28, 1))\n",
    "print(fully_connected_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so we have 472,042 parameters and four hidden layers. Let's see how well our model performs after some training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_connected_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6013 (pid 5720), started 17:32:11 ago. (Use '!kill 5720' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d34bb9f93d4e0319\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d34bb9f93d4e0319\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6013;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "  1/394 [..............................] - ETA: 0s - loss: 2.3075 - accuracy: 0.1016WARNING:tensorflow:From /Users/heckel/opt/anaconda3/envs/MNIST/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0146s). Check your callbacks.\n",
      "394/394 [==============================] - 2s 4ms/step - loss: 0.2860 - accuracy: 0.9152 - val_loss: 0.1519 - val_accuracy: 0.9545\n",
      "Epoch 2/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.1025 - accuracy: 0.9685 - val_loss: 0.0990 - val_accuracy: 0.9702\n",
      "Epoch 3/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0648 - accuracy: 0.9804 - val_loss: 0.0866 - val_accuracy: 0.9730\n",
      "Epoch 4/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0454 - accuracy: 0.9858 - val_loss: 0.0795 - val_accuracy: 0.9760\n",
      "Epoch 5/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0340 - accuracy: 0.9890 - val_loss: 0.0848 - val_accuracy: 0.9758\n",
      "Epoch 6/15\n",
      "394/394 [==============================] - 2s 4ms/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 0.0807 - val_accuracy: 0.9777\n",
      "Epoch 7/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0202 - accuracy: 0.9930 - val_loss: 0.0758 - val_accuracy: 0.9800\n",
      "Epoch 8/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 0.0909 - val_accuracy: 0.9772\n",
      "Epoch 9/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0156 - accuracy: 0.9947 - val_loss: 0.1000 - val_accuracy: 0.9755\n",
      "Epoch 10/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0164 - accuracy: 0.9942 - val_loss: 0.0797 - val_accuracy: 0.9799\n",
      "Epoch 11/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.0888 - val_accuracy: 0.9800\n",
      "Epoch 12/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0121 - accuracy: 0.9961 - val_loss: 0.0839 - val_accuracy: 0.9814\n",
      "Epoch 13/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0103 - accuracy: 0.9965 - val_loss: 0.0908 - val_accuracy: 0.9801\n",
      "Epoch 14/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0099 - accuracy: 0.9968 - val_loss: 0.1076 - val_accuracy: 0.9793\n",
      "Epoch 15/15\n",
      "394/394 [==============================] - 1s 4ms/step - loss: 0.0088 - accuracy: 0.9972 - val_loss: 0.1060 - val_accuracy: 0.9793\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9811\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fully_connexted_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3b475072d24d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfully_connected_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfully_connexted_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fully_connexted_model' is not defined"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir ./logs/fully_connected_model/\n",
    "history = fully_connected_model.fit(x_train, y_train, batch_size=128, epochs=15, validation_split=0.16, shuffle=True, \n",
    "                    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./logs/fully_connected_model/')])\n",
    "score = fully_connected_model.evaluate(x_test, y_test)\n",
    "\n",
    "fully_connexted_model.save('./models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does our model perform? Let's have a look at the graphs and scores (if you are using tensorboard, you have seen the graphs already)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy on train set: ', history.history['accuracy'][-1])\n",
    "print('Accuracy on test set: ', score[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 98.2% accuracy on the test set which is already quite good. On the train set we achieve 99.5% accuracy which is much more. So our model seems to overfit the training set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a brief look at examples the model was not predicting correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_incorrect_preds(model):\n",
    "    pred = model.predict(x_test)\n",
    "    pred_labels = np.argmax(pred, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "    incorrect = np.where(pred_labels != true_labels)[0]\n",
    "    print('Number of wrong predictions: ', len(incorrect))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 8, figsize=(16, 2))\n",
    "    for ind, ax in enumerate(axes.flat):\n",
    "        ind = incorrect[ind]\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(x_test[ind], cmap=plt.get_cmap('gray'))\n",
    "        true_label = one_hot_to_label(y_test[ind])\n",
    "        predicted_label = one_hot_to_label(pred[ind])\n",
    "        confidence = pred[ind, predicted_label]\n",
    "        ax.set_title('Label: %s\\nPrediction: %s\\nConfidence %1.2f' %(true_label, predicted_label, confidence))\n",
    "        \n",
    "plot_incorrect_preds(fully_connected_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows 176 wrong predictions in total. At least among our examples, some of them even have a very high confidence.\n",
    "Let us try to see if we can discover some systematic error in our models prediction. To to so, we plot predictions versus ground truth and inicate the number of samples at a point (truth, prediction) by its color itensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds_scatter(model):\n",
    "    pred = model.predict(x_test)\n",
    "    pred_labels = np.argmax(pred, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    df = pd.DataFrame(true_labels, columns=['Truth'])\n",
    "    df['Prediction'] = pred_labels\n",
    "    df['Incorrect'] = np.where(df['Truth'] == df['Prediction'], False, True)\n",
    "    df_incorr = df.loc[df.Incorrect].reset_index(drop=True)\n",
    "    \n",
    "    df_incorr.plot(x='Truth', y='Prediction', kind='scatter', alpha=.1, s=50, title='')\n",
    "    \n",
    "plot_preds_scatter(fully_connected_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reveals, for example, that a two is often mistaken for an eight by the model, a three for a five, and also a nine for a five. We may also have a look at how often number are predicted incorrectly and how many incorrect predictions are there per number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_incorr_preds_hist(model):\n",
    "    pred = model.predict(x_test)\n",
    "    pred_labels = np.argmax(pred, axis=1)\n",
    "    true_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    df = pd.DataFrame(true_labels, columns=['Truth'])\n",
    "    df['Prediction'] = pred_labels\n",
    "    df['Incorrect'] = np.where(df['Truth'] == df['Prediction'], False, True)\n",
    "    df_incorr = df.loc[df.Incorrect].reset_index(drop=True)\n",
    "    \n",
    "    df_incorr.hist(column=['Truth', 'Prediction'], figsize=(16, 4), grid=False)\n",
    "\n",
    "plot_incorr_preds_hist(fully_connected_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers three and nine are seen to cause most of the mispredictions but the model also isn't doing too well on three, seven and eight. If a seven is predicted, this is more often false than for other numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A convolutional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computer vision tasks (as recognizing digits in images) convolutional networks usually perform better as it is possible for them to learn patterns. So let us set up a new model with one convolutional layer followed by dense layers, reducing the the size by four in each step, just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def conv_model(input_shape, name='Convolutional_MNIST_model'):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    X = Conv2D(32, (4, 4), strides=2)(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same')(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1024, activation='relu')(X)\n",
    "    X = Dense(256, activation='relu')(X)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dense(10, activation='softmax')(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(X_input, X, name=name)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "conv_model = conv_model((28, 28, 1))\n",
    "print(conv_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./logs/conv_model/\n",
    "    \n",
    "history = conv_model.fit(x_train, y_train, batch_size=128, epochs=15, validation_split=0.16, shuffle=True,\n",
    "                               callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./logs/conv_model/')])\n",
    "score = conv_model.evaluate(x_test, y_test)\n",
    "\n",
    "conv_model.save('./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train set: ', history.history['accuracy'][-1])\n",
    "print('Accuracy on test set: ', score[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional model performs a little better on the test set but also clearly overfits the train set. One possibility to reduce overfitting that usually works is to add dropout layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model is basically the same as before but containing three dropout layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_with_dropout(input_shape, name='Convolutional_MNIST_model_with_dropout'):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    X = Conv2D(32, (4, 4), strides=2, activation='relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same')(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1024, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(256, activation='relu')(X)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(10, activation='softmax')(X)\n",
    "    \n",
    "    model = Model(X_input, X, name=name)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "conv_model_with_dropout = conv_model_with_dropout((28, 28, 1))\n",
    "print(conv_model_with_dropout.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model_with_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "%tensorboard --logdir ./logs/conv_model_with_dropout/\n",
    "    \n",
    "history = conv_model_with_dropout.fit(x_train, y_train, batch_size=128, epochs=10, validation_split=0.16, \n",
    "                                      shuffle=True,\n",
    "                                      callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./logs/conv_model_with_dropout/')])\n",
    "score = conv_model_with_dropout.evaluate(x_test, y_test)\n",
    "\n",
    "conv_model_with_dropout.save('./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train set: ', history.history['accuracy'][-1])\n",
    "print('Accuracy on test set: ', score[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is doing well on the train set and even a little better on the test set. The difference in performance on both is quite small so we can say that we successfully reduced overfitting of our model. We may look again at mislabelled examples and do our short error analysis as we did for the fully connected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_incorrect_preds(conv_model_with_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_preds_scatter(conv_model_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest issues seem to be that a nine is oftentimes predicted as four and a two as seven. Let's see if the histograms confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_incorr_preds_hist(conv_model_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They definetely do! So for improving this model it could be a good idea to train it more images of nine and two which we can easily get data agumentation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is also a quick way to improve model performance. Let us see if it can help our previous model to perform even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_dropout_batchnorm(input_shape, name='Convolutional_MNIST_model_with_dropout_and_batch_normalization'):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    X = BatchNormalization()(X)\n",
    "    X = Conv2D(32, (4, 4), strides=2, activation='relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same')(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    \n",
    "    X = BatchNormalization()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(1024, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(256, activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(10, activation='softmax')(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(X_input, X, name=name)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "conv_model_dropout_batchnorm = conv_model_dropout_batchnorm((28, 28, 1))\n",
    "print(conv_model_dropout_batchnorm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model_dropout_batchnorm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "%tensorboard --logdir ./logs/conv_model_dropout_batchnorm/\n",
    "    \n",
    "history = conv_model_dropout_batchnorm.fit(x_train, y_train, batch_size=128, epochs=15, validation_split=0.16, \n",
    "                                          shuffle=True,\n",
    "                                          callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./logs/conv_model_dropout_batchnorm/')])\n",
    "score = conv_model_dropout_batchnorm.evaluate(x_test, y_test)\n",
    "\n",
    "conv_model_dropout_batchnorm.save('./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy on train set: ', history.history['accuracy'][-1])\n",
    "print('Accuracy on test set: ', score[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is almost no performance improvement, instead our model tends more to overfit the training data. Thus, batch normalization doesn't seem to be helpful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A deep convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As introducing a single convolutional layer worked out well, let us see how things are if we add many convolutional layer and try to learn the digit classification with a deep convolutional network. We will continue using dropout to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_conv_model(input_shape, name='Deep_convolutional_MNIST_model'):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = X_input\n",
    "    \n",
    "    X = Conv2D(8, (4, 4), activation='relu', padding='same')(X)\n",
    "    X = Conv2D(8, (1, 1), activation='relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(2, 2), strides=2, padding='same')(X)\n",
    "    X = Conv2D(16, (4, 4), activation='relu', padding='same')(X)\n",
    "    X = Conv2D(16, (1, 1), activation='relu')(X)\n",
    "    X = Conv2D(32, (4, 4), strides=2, activation='relu', padding='same')(X)\n",
    "    X = Conv2D(32, (1, 1), activation='relu')(X)\n",
    "    X = Conv2D(64, (4, 4), strides=2, activation='relu', padding='same')(X)\n",
    "    X = Conv2D(64, (1, 1), activation='relu')(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = Conv2D(128, (4, 4), strides=2, activation='relu')(X)\n",
    "    X = Conv2D(128, (1, 1), activation='relu')(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    \n",
    "    X = Flatten()(X)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = Dense(16, activation='relu')(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(10, activation='softmax')(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(X_input, X, name=name)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "deep_conv_model = deep_conv_model((28, 28, 1))\n",
    "print(deep_conv_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that altough this is a deep model, it has much fewer parameters than our previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_conv_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "%tensorboard --logdir ./logs/deep_conv_model/\n",
    "    \n",
    "history = deep_conv_model.fit(x_train, y_train, batch_size=128, epochs=15, validation_split=0.16, shuffle=True,\n",
    "                              callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./logs/deep_conv_model/')])\n",
    "score = deep_conv_model.evaluate(x_test, y_test)\n",
    "\n",
    "deep_conv_model.save('./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy on train set: ', history.history['accuracy'][-1])\n",
    "print('Accuracy on test set: ', score[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is 99% percent accuracy and the best we had so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_incorrect_preds(deep_conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_preds_scatter(deep_conv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_incorr_preds_hist(deep_conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reached 99% percent accuracy without any data augmentation or advanced techniques and also have shown some ways how the model could be improved. 99% success is quite a success for a model on the MNIST dataset, in particular for networks without any fine-tuning or extensive data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
